{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import所需库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# bs4 nltk gensim\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 用pandas读入训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 25000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \"The Classic War of the Worlds\" by Timothy Hin...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/labeledTrainData.tsv', sep='\\t', escapechar='\\\\')\n",
    "print('Number of reviews: {}'.format(len(df)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 对影评数据做预处理，大概有以下环节：\n",
    "\n",
    "1. 去掉html标签\n",
    "1. 移除标点\n",
    "1. 切分成词/token\n",
    "1. 去掉停用词\n",
    "1. 重组为新的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I watched this movie really late last night and usually if it's late then I'm pretty forgiving of movies. Although I tried, I just could not stand this movie at all, it kept getting worse and worse as the movie went on. Although I know it's suppose to be a comedy but I didn't find it very funny. It was also an especially unrealistic, and jaded portrayal of rural life. In case this is what any of you think country life is like, it's definitely not. I do have to agree that some of the guy cast members were cute, but the french guy was really fake. I do have to agree that it tried to have a good lesson in the story, but overall my recommendation is that no one over 8 watch it, it's just too annoying.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I watched this movie really late last night and usually if it's late then I'm pretty forgiving of movies. Although I tried, I just could not stand this movie at all, it kept getting worse and worse as the movie went on. Although I know it's suppose to be a comedy but I didn't find it very funny. It was also an especially unrealistic, and jaded portrayal of rural life. In case this is what any of you think country life is like, it's definitely not. I do have to agree that some of the guy cast members were cute, but the french guy was really fake. I do have to agree that it tried to have a good lesson in the story, but overall my recommendation is that no one over 8 watch it, it's just too annoying.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#去掉HTML标签的数据\n",
    "example = BeautifulSoup(df['review'][1000], 'html.parser').get_text()\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I watched this movie really late last night and usually if it s late then I m pretty forgiving of movies  Although I tried  I just could not stand this movie at all  it kept getting worse and worse as the movie went on  Although I know it s suppose to be a comedy but I didn t find it very funny  It was also an especially unrealistic  and jaded portrayal of rural life  In case this is what any of you think country life is like  it s definitely not  I do have to agree that some of the guy cast members were cute  but the french guy was really fake  I do have to agree that it tried to have a good lesson in the story  but overall my recommendation is that no one over   watch it  it s just too annoying '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#去掉标点符号\n",
    "example_letters = re.sub(r'[^a-zA-Z]', ' ', example)\n",
    "example_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'watched',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'really',\n",
       " 'late',\n",
       " 'last',\n",
       " 'night',\n",
       " 'and',\n",
       " 'usually',\n",
       " 'if',\n",
       " 'it',\n",
       " 's',\n",
       " 'late',\n",
       " 'then',\n",
       " 'i',\n",
       " 'm',\n",
       " 'pretty',\n",
       " 'forgiving',\n",
       " 'of',\n",
       " 'movies',\n",
       " 'although',\n",
       " 'i',\n",
       " 'tried',\n",
       " 'i',\n",
       " 'just',\n",
       " 'could',\n",
       " 'not',\n",
       " 'stand',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'at',\n",
       " 'all',\n",
       " 'it',\n",
       " 'kept',\n",
       " 'getting',\n",
       " 'worse',\n",
       " 'and',\n",
       " 'worse',\n",
       " 'as',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'went',\n",
       " 'on',\n",
       " 'although',\n",
       " 'i',\n",
       " 'know',\n",
       " 'it',\n",
       " 's',\n",
       " 'suppose',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'comedy',\n",
       " 'but',\n",
       " 'i',\n",
       " 'didn',\n",
       " 't',\n",
       " 'find',\n",
       " 'it',\n",
       " 'very',\n",
       " 'funny',\n",
       " 'it',\n",
       " 'was',\n",
       " 'also',\n",
       " 'an',\n",
       " 'especially',\n",
       " 'unrealistic',\n",
       " 'and',\n",
       " 'jaded',\n",
       " 'portrayal',\n",
       " 'of',\n",
       " 'rural',\n",
       " 'life',\n",
       " 'in',\n",
       " 'case',\n",
       " 'this',\n",
       " 'is',\n",
       " 'what',\n",
       " 'any',\n",
       " 'of',\n",
       " 'you',\n",
       " 'think',\n",
       " 'country',\n",
       " 'life',\n",
       " 'is',\n",
       " 'like',\n",
       " 'it',\n",
       " 's',\n",
       " 'definitely',\n",
       " 'not',\n",
       " 'i',\n",
       " 'do',\n",
       " 'have',\n",
       " 'to',\n",
       " 'agree',\n",
       " 'that',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'guy',\n",
       " 'cast',\n",
       " 'members',\n",
       " 'were',\n",
       " 'cute',\n",
       " 'but',\n",
       " 'the',\n",
       " 'french',\n",
       " 'guy',\n",
       " 'was',\n",
       " 'really',\n",
       " 'fake',\n",
       " 'i',\n",
       " 'do',\n",
       " 'have',\n",
       " 'to',\n",
       " 'agree',\n",
       " 'that',\n",
       " 'it',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'have',\n",
       " 'a',\n",
       " 'good',\n",
       " 'lesson',\n",
       " 'in',\n",
       " 'the',\n",
       " 'story',\n",
       " 'but',\n",
       " 'overall',\n",
       " 'my',\n",
       " 'recommendation',\n",
       " 'is',\n",
       " 'that',\n",
       " 'no',\n",
       " 'one',\n",
       " 'over',\n",
       " 'watch',\n",
       " 'it',\n",
       " 'it',\n",
       " 's',\n",
       " 'just',\n",
       " 'too',\n",
       " 'annoying']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "words = example_letters.lower().split()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#下载停用词和其他语料会用到\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watched',\n",
       " 'movie',\n",
       " 'late',\n",
       " 'night',\n",
       " 'late',\n",
       " 'pretty',\n",
       " 'forgiving',\n",
       " 'movies',\n",
       " 'stand',\n",
       " 'movie',\n",
       " 'worse',\n",
       " 'worse',\n",
       " 'movie',\n",
       " 'suppose',\n",
       " 'comedy',\n",
       " 'didn',\n",
       " 'funny',\n",
       " 'unrealistic',\n",
       " 'jaded',\n",
       " 'portrayal',\n",
       " 'rural',\n",
       " 'life',\n",
       " 'country',\n",
       " 'life',\n",
       " 'agree',\n",
       " 'guy',\n",
       " 'cast',\n",
       " 'cute',\n",
       " 'french',\n",
       " 'guy',\n",
       " 'fake',\n",
       " 'agree',\n",
       " 'lesson',\n",
       " 'story',\n",
       " 'recommendation',\n",
       " 'watch',\n",
       " 'annoying']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#去停用词\n",
    "stopwords = {}.fromkeys([ line.rstrip() for line in open('./stopwords.txt')])\n",
    "words_nostop = [w for w in words if w not in stopwords]\n",
    "words_nostop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = set(stopwords)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "    words = [w for w in words if w not in eng_stopwords]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I watched this movie really late last night and usually if it's late then I'm pretty forgiving of movies. Although I tried, I just could not stand this movie at all, it kept getting worse and worse as the movie went on. Although I know it's suppose to be a comedy but I didn't find it very funny. It was also an especially unrealistic, and jaded portrayal of rural life. In case this is what any of you think country life is like, it's definitely not. I do have to agree that some of the guy cast members were cute, but the french guy was really fake. I do have to agree that it tried to have a good lesson in the story, but overall my recommendation is that no one over 8 watch it, it's just too annoying.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watched movie late night late pretty forgiving movies stand movie worse worse movie suppose comedy didn funny unrealistic jaded portrayal rural life country life agree guy cast cute french guy fake agree lesson story recommendation watch annoying'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(df['review'][1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清洗数据添加到dataframe里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>stuff moment mj ve started listening music wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "      <td>classic war worlds timothy hines entertaining ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "      <td>film starts manager nicholas bell investors ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "      <td>assumed praised film filmed opera didn read do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "      <td>superbly trashy wondrously unpretentious explo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review  \\\n",
       "0  5814_8          1  With all this stuff going down at the moment w...   \n",
       "1  2381_9          1  \"The Classic War of the Worlds\" by Timothy Hin...   \n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...   \n",
       "3  3630_4          0  It must be assumed that those who praised this...   \n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...   \n",
       "\n",
       "                                        clean_review  \n",
       "0  stuff moment mj ve started listening music wat...  \n",
       "1  classic war worlds timothy hines entertaining ...  \n",
       "2  film starts manager nicholas bell investors ro...  \n",
       "3  assumed praised film filmed opera didn read do...  \n",
       "4  superbly trashy wondrously unpretentious explo...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_review'] = df.review.apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 抽取bag of words特征(用sklearn的CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features = 5000) \n",
    "train_data_features = vectorizer.fit_transform(df.clean_review).toarray()\n",
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data_features,df.sentiment,test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall metric in the testing dataset:  0.8507340946166395\n",
      "accuracy metric in the testing dataset:  0.845\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "LR_model = LogisticRegression()\n",
    "LR_model = LR_model.fit(X_train, y_train)\n",
    "y_pred = LR_model.predict(X_test)\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "print(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "\n",
    "print(\"accuracy metric in the testing dataset: \", (cnf_matrix[1,1]+cnf_matrix[0,0])/(cnf_matrix[0,0]+cnf_matrix[1,1]+cnf_matrix[1,0]+cnf_matrix[0,1]))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names\n",
    "                      , title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9999_0</td>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45057_0</td>\n",
       "      <td>I saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15561_0</td>\n",
       "      <td>Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7161_0</td>\n",
       "      <td>I went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43971_0</td>\n",
       "      <td>Yes, I agree with everyone on this site this m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             review\n",
       "0   9999_0  Watching Time Chasers, it obvious that it was ...\n",
       "1  45057_0  I saw this film about 20 years ago and remembe...\n",
       "2  15561_0  Minor Spoilers<br /><br />In New York, Joan Ba...\n",
       "3   7161_0  I went to see this film with a great deal of e...\n",
       "4  43971_0  Yes, I agree with everyone on this site this m..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/unlabeledTrainData.tsv', sep='\\t', escapechar='\\\\')\n",
    "print('Number of reviews: {}'.format(len(df)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9999_0</td>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "      <td>watching time chasers obvious bunch friends si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45057_0</td>\n",
       "      <td>I saw this film about 20 years ago and remembe...</td>\n",
       "      <td>film ago remember nasty based true incident br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15561_0</td>\n",
       "      <td>Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan Ba...</td>\n",
       "      <td>minor spoilersin york joan barnard elvire audr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7161_0</td>\n",
       "      <td>I went to see this film with a great deal of e...</td>\n",
       "      <td>film deal excitement school director friend mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43971_0</td>\n",
       "      <td>Yes, I agree with everyone on this site this m...</td>\n",
       "      <td>agree site movie bad call movie insult movies ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             review  \\\n",
       "0   9999_0  Watching Time Chasers, it obvious that it was ...   \n",
       "1  45057_0  I saw this film about 20 years ago and remembe...   \n",
       "2  15561_0  Minor Spoilers<br /><br />In New York, Joan Ba...   \n",
       "3   7161_0  I went to see this film with a great deal of e...   \n",
       "4  43971_0  Yes, I agree with everyone on this site this m...   \n",
       "\n",
       "                                        clean_review  \n",
       "0  watching time chasers obvious bunch friends si...  \n",
       "1  film ago remember nasty based true incident br...  \n",
       "2  minor spoilersin york joan barnard elvire audr...  \n",
       "3  film deal excitement school director friend mi...  \n",
       "4  agree site movie bad call movie insult movies ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_review'] = df.review.apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_part = df['clean_review']\n",
    "review_part.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 reviews -> 50000 sentences\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "\n",
    "def split_sentences(review):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = [clean_text(s) for s in raw_sentences if s]\n",
    "    return sentences\n",
    "sentences = sum(review_part.apply(split_sentences), [])\n",
    "print('{} reviews -> {} sentences'.format(len(review_part), len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watching time chasers obvious bunch friends sitting day film school hey pool money bad movie bad movie dull story bad script lame acting poor cinematography bottom barrel stock music corners cut prevented film release life'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/zl/nltk_data'\n    - '/Users/zl/anaconda3/nltk_data'\n    - '/Users/zl/anaconda3/share/nltk_data'\n    - '/Users/zl/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-91895377fcf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentences_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msentences_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \"\"\"\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     return [\n\u001b[1;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/zl/nltk_data'\n    - '/Users/zl/anaconda3/nltk_data'\n    - '/Users/zl/anaconda3/share/nltk_data'\n    - '/Users/zl/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "sentences_list = []\n",
    "for line in sentences:\n",
    "    sentences_list.append(nltk.word_tokenize(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  sentences：可以是一个list\n",
    "-  sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法。\n",
    "-  size：是指特征向量的维度，默认为100。大的size需要更多的训练数据,但是效果会更好. 推荐值为几十到几百。\n",
    "-  window：表示当前词与预测词在一个句子中的最大距离是多少\n",
    "-  alpha: 是学习速率\n",
    "-  seed：用于随机数发生器。与初始化词向量有关。\n",
    "-  min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5\n",
    "-  max_vocab_size: 设置词向量构建期间的RAM限制。如果所有独立单词个数超过这个，则就消除掉其中最不频繁的一个。每一千万个单词需要大约1GB的RAM。设置成None则没有限制。\n",
    "\n",
    "-  workers参数控制训练的并行数。\n",
    "-  hs: 如果为1则会采用hierarchica·softmax技巧。如果设置为0（defau·t），则negative sampling会被使用。\n",
    "-  negative: 如果>0,则会采用negativesamp·ing，用于设置多少个noise words\n",
    "-  iter： 迭代次数，默认为5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定词向量训练的参数\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 40   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "model_name = '{}features_{}minwords_{}context.model'.format(num_features, min_word_count, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-9f54d81fe3da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_word_count\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# If you don't plan to train the model any further, calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# init_sims will make the model much more memory-efficient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# should be set by `build_vocab`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(sentences_list, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model.save(os.path.join('..', 'models', model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.doesnt_match(['man','woman','child','kitchen']))\n",
    "#print(model.doesnt_match('france england germany berlin'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/labeledTrainData.tsv', sep='\\t', escapechar='\\\\')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text, remove_stopwords=False):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if w not in eng_stopwords]\n",
    "    return words\n",
    "\n",
    "def to_review_vector(review):\n",
    "    global word_vec\n",
    "    \n",
    "    review = clean_text(review, remove_stopwords=True)\n",
    "    #print (review)\n",
    "    #words = nltk.word_tokenize(review)\n",
    "    word_vec = np.zeros((1,300))\n",
    "    for word in review:\n",
    "        #word_vec = np.zeros((1,300))\n",
    "        if word in model:\n",
    "            word_vec += np.array([model[word]])\n",
    "    #print (word_vec.mean(axis = 0))\n",
    "    return pd.Series(word_vec.mean(axis = 0))\n",
    "\n",
    "train_data_features = df.review.apply(to_review_vector)\n",
    "train_data_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data_features,df.sentiment,test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model = LogisticRegression()\n",
    "LR_model = LR_model.fit(X_train, y_train)\n",
    "y_pred = LR_model.predict(X_test)\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "print(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "\n",
    "print(\"accuracy metric in the testing dataset: \", (cnf_matrix[1,1]+cnf_matrix[0,0])/(cnf_matrix[0,0]+cnf_matrix[1,1]+cnf_matrix[1,0]+cnf_matrix[0,1]))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names\n",
    "                      , title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
